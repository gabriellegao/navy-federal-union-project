{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Preprocessing Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, time, pandas, numpy, enchant\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "enchant_dict = enchant.Dict(\"en_US\")\n",
    "stopwords = list(stopwords.words('english'))+list(string.punctuation)\n",
    "tokenizer = TweetTokenizer()\n",
    "url_re = re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+|www.[^ ]+',re.VERBOSE | re.IGNORECASE)\n",
    "mention_re = r'@[A-Za-z0-9]+'\n",
    "hashtag_re = r'#[A-Za-z0-9]+'\n",
    "\n",
    "def denoise(raw_tweet):\n",
    "    if type(raw_tweet)==list:\n",
    "        return [denoise(t) for t in raw_tweet]\n",
    "    # Remove URLs and mentions\n",
    "    raw_tweet = re.sub(url_re, ' ', raw_tweet)\n",
    "    raw_tweet = re.sub(mention_re, ' ', raw_tweet)\n",
    "    # TODO: anything with hashtags?\n",
    "    # TODO: number normalization? (see normalize_number_todo.txt)\n",
    "    # TODO: condense contractions?\n",
    "    return raw_tweet\n",
    "\n",
    "def tokenize(raw_tweet, stopwords=stopwords):\n",
    "    if type(raw_tweet)==list:\n",
    "        return [tokenize(t) for t in raw_tweet]\n",
    "    return [token.lower() for token in tokenizer.tokenize(raw_tweet) if len(token)>0 and token.lower() not in stopwords]\n",
    "\n",
    "def lemmatize(tokens, stopwords=stopwords):\n",
    "    if len(tokens)!=0 and type(tokens[0])==list:\n",
    "        return [lemmatize(t) for t in tokens]\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        #stemmer = PorterStemmer()\n",
    "        #token = stemmer.stem(token)\n",
    "        if len(token)>0 and token.lower() not in stopwords:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, pos))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def only_english(tokens):\n",
    "    if len(tokens)!=0 and type(tokens[0])==list:\n",
    "        return [only_english(t) for t in tokens]\n",
    "    return [e for e in tokens if enchant_dict.check(e)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Feature Extraction Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def vectorize(tokenized_tweets):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy_fun,\n",
    "        preprocessor=dummy_fun,\n",
    "        token_pattern=None,\n",
    "        min_df=10,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1,2)\n",
    "    )\n",
    "\n",
    "    tfidf.fit(tokenized_tweets)\n",
    "    return tfidf\n",
    "\n",
    "def extract_polarity_features(dataset):\n",
    "    #Split tweets into training and test sets with even portions of positive to negative tweets\n",
    "    df = dataset.drop(dataset[dataset.airline_sentiment==\"neutral\"].index)\n",
    "    \n",
    "    positive_tweets = df[df.airline_sentiment==\"positive\"]\n",
    "    negative_tweets = df[df.airline_sentiment==\"negative\"]\n",
    "\n",
    "    positive_tweets_train = positive_tweets.sample(frac=0.5)\n",
    "    positive_tweets_test = positive_tweets.drop(positive_tweets_train.index)\n",
    "\n",
    "    negative_tweets_train = negative_tweets.sample(frac=0.5)\n",
    "    negative_tweets_test = negative_tweets.drop(negative_tweets_train.index)\n",
    "\n",
    "    tweets_train = pandas.concat([positive_tweets_train, negative_tweets_train])\n",
    "    tweets_test = pandas.concat([positive_tweets_test, negative_tweets_test])\n",
    "    tfidf=vectorize(dataset.tokens)\n",
    "    X_train = tfidf.transform(tweets_train.lemmatized_tokens).toarray()\n",
    "    y_train = [1 if label==\"positive\" else 0 for label in tweets_train.airline_sentiment]\n",
    "    X_test = tfidf.transform(tweets_test.lemmatized_tokens).toarray()\n",
    "    y_test = [1 if label==\"positive\" else 0 for label in tweets_test.airline_sentiment]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def extract_subjectivity_features(dataset):\n",
    "    df = dataset.assign(subjective=dataset.airline_sentiment.apply(lambda s: 0 if s==\"neutral\" else 1))\n",
    "    subjective_tweets = df[df.subjective==1]\n",
    "    objective_tweets = df[df.subjective==0]\n",
    "\n",
    "    subjective_tweets_train = subjective_tweets.sample(frac=0.8)\n",
    "    subjective_tweets_test = subjective_tweets.drop(subjective_tweets_train.index)\n",
    "\n",
    "    objective_tweets_train = objective_tweets.sample(frac=0.8)\n",
    "    objective_tweets_test = objective_tweets.drop(objective_tweets_train.index)\n",
    "\n",
    "    tweets_train = pandas.concat([subjective_tweets_train, objective_tweets_train])\n",
    "    tweets_test = pandas.concat([subjective_tweets_test, objective_tweets_test])\n",
    "    tfidf=vectorize(dataset.tokens)\n",
    "    X_train = tfidf.transform(tweets_train.lemmatized_tweet)\n",
    "    y_train = tweets_train.subjective\n",
    "    X_test = tfidf.transform(tweets_test.lemmatized_tweet).toarray()\n",
    "    y_test = tweets_test.subjective\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Load dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 89.28535342216492 seconds to load and preprocess tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>denoised_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>english_lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>What   said.</td>\n",
       "      <td>[said]</td>\n",
       "      <td>[say]</td>\n",
       "      <td>[say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>plus you've added commercials to the experie...</td>\n",
       "      <td>[plus, added, commercials, experience, ..., ta...</td>\n",
       "      <td>[plus, added, commercial, experience, ..., tacky]</td>\n",
       "      <td>[plus, added, commercial, experience, ..., tacky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>I didn't today... Must mean I need to take a...</td>\n",
       "      <td>[today, ..., must, mean, need, take, another, ...</td>\n",
       "      <td>[today, ..., must, mean, need, take, another, ...</td>\n",
       "      <td>[today, ..., must, mean, need, take, another, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"e...</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, enterta...</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, enterta...</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you we got on a different flight to Ch...</td>\n",
       "      <td>[thank, got, different, flight, chicago]</td>\n",
       "      <td>[thank, get, different, flight, chicago]</td>\n",
       "      <td>[thank, get, different, flight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaving over 20 minutes Late Flight. No warn...</td>\n",
       "      <td>[leaving, 20, minutes, late, flight, warnings,...</td>\n",
       "      <td>[leave, 20, minute, late, flight, warning, com...</td>\n",
       "      <td>[leave, 20, minute, late, flight, warning, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please bring American Airlines to #BlackBerry10</td>\n",
       "      <td>[please, bring, american, airlines, #blackberr...</td>\n",
       "      <td>[please, bring, american, airline, #blackberry10]</td>\n",
       "      <td>[please, bring, airline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>you have my money, you change my flight, and...</td>\n",
       "      <td>[money, change, flight, answer, phones, sugges...</td>\n",
       "      <td>[money, change, flight, answer, phone, suggest...</td>\n",
       "      <td>[money, change, flight, answer, phone, suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we have 8 ppl so we need 2 know how many sea...</td>\n",
       "      <td>[8, ppl, need, 2, know, many, seats, next, fli...</td>\n",
       "      <td>[8, ppl, need, 2, know, many, seat, next, flig...</td>\n",
       "      <td>[8, need, 2, know, many, seat, next, flight, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "0                  0                @VirginAmerica What @dhepburn said.   \n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \\\n",
       "0      Eastern Time (US & Canada)   \n",
       "1      Pacific Time (US & Canada)   \n",
       "2      Central Time (US & Canada)   \n",
       "3      Pacific Time (US & Canada)   \n",
       "4      Pacific Time (US & Canada)   \n",
       "...                           ...   \n",
       "14635                         NaN   \n",
       "14636                         NaN   \n",
       "14637                         NaN   \n",
       "14638  Eastern Time (US & Canada)   \n",
       "14639                         NaN   \n",
       "\n",
       "                                           denoised_text  \\\n",
       "0                                           What   said.   \n",
       "1        plus you've added commercials to the experie...   \n",
       "2        I didn't today... Must mean I need to take a...   \n",
       "3        it's really aggressive to blast obnoxious \"e...   \n",
       "4               and it's a really big bad thing about it   \n",
       "...                                                  ...   \n",
       "14635    thank you we got on a different flight to Ch...   \n",
       "14636    leaving over 20 minutes Late Flight. No warn...   \n",
       "14637    Please bring American Airlines to #BlackBerry10   \n",
       "14638    you have my money, you change my flight, and...   \n",
       "14639    we have 8 ppl so we need 2 know how many sea...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0                                                 [said]   \n",
       "1      [plus, added, commercials, experience, ..., ta...   \n",
       "2      [today, ..., must, mean, need, take, another, ...   \n",
       "3      [really, aggressive, blast, obnoxious, enterta...   \n",
       "4                              [really, big, bad, thing]   \n",
       "...                                                  ...   \n",
       "14635           [thank, got, different, flight, chicago]   \n",
       "14636  [leaving, 20, minutes, late, flight, warnings,...   \n",
       "14637  [please, bring, american, airlines, #blackberr...   \n",
       "14638  [money, change, flight, answer, phones, sugges...   \n",
       "14639  [8, ppl, need, 2, know, many, seats, next, fli...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "0                                                  [say]   \n",
       "1      [plus, added, commercial, experience, ..., tacky]   \n",
       "2      [today, ..., must, mean, need, take, another, ...   \n",
       "3      [really, aggressive, blast, obnoxious, enterta...   \n",
       "4                              [really, big, bad, thing]   \n",
       "...                                                  ...   \n",
       "14635           [thank, get, different, flight, chicago]   \n",
       "14636  [leave, 20, minute, late, flight, warning, com...   \n",
       "14637  [please, bring, american, airline, #blackberry10]   \n",
       "14638  [money, change, flight, answer, phone, suggest...   \n",
       "14639  [8, ppl, need, 2, know, many, seat, next, flig...   \n",
       "\n",
       "                               english_lemmatized_tokens  \n",
       "0                                                  [say]  \n",
       "1      [plus, added, commercial, experience, ..., tacky]  \n",
       "2      [today, ..., must, mean, need, take, another, ...  \n",
       "3      [really, aggressive, blast, obnoxious, enterta...  \n",
       "4                              [really, big, bad, thing]  \n",
       "...                                                  ...  \n",
       "14635                    [thank, get, different, flight]  \n",
       "14636  [leave, 20, minute, late, flight, warning, com...  \n",
       "14637                           [please, bring, airline]  \n",
       "14638  [money, change, flight, answer, phone, suggest...  \n",
       "14639  [8, need, 2, know, many, seat, next, flight, p...  \n",
       "\n",
       "[14640 rows x 19 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = pandas.read_csv('https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv', encoding='latin-1')\n",
    "dataset = dataset.assign(denoised_text = denoise(list(dataset.text.astype(str))))\n",
    "dataset = dataset.assign(tokens = tokenize(list(dataset.denoised_text.astype(str))))\n",
    "dataset = dataset.assign(lemmatized_tokens = lemmatize(list(dataset.tokens)))\n",
    "dataset = dataset.assign(english_lemmatized_tokens = only_english(list(dataset.lemmatized_tokens)))\n",
    "print(\"Took \" + str(time.time()-start) + \" seconds to load and preprocess tweets\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Polarity classifier (positive vs. negative)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4531   58]\n",
      " [ 521  660]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      4589\n",
      "           1       0.92      0.56      0.70      1181\n",
      "\n",
      "    accuracy                           0.90      5770\n",
      "   macro avg       0.91      0.77      0.82      5770\n",
      "weighted avg       0.90      0.90      0.89      5770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X_train, y_train, X_test, y_test = extract_polarity_features(dataset)\n",
    "\n",
    "polarity_classifier = LogisticRegression()\n",
    "polarity_classifier.fit(X_train, y_train)\n",
    "y_pred = polarity_classifier.predict(X_test)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "#TODO: error?\n",
    "print(confusion_mat, classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Subjectivity classifier (objective vs. subjective)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lemmatized_tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-e1d4599c34fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_subjectivity_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msubjectivity_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-52dbf4a03e7e>\u001b[0m in \u001b[0;36mextract_subjectivity_features\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtweets_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubjective_tweets_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective_tweets_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubjective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\swein\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lemmatized_tweet'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X_train, y_train, X_test, y_test = extract_subjectivity_features(dataset)\n",
    "\n",
    "subjectivity_classifier = LogisticRegression()\n",
    "subjectivity_classifier.fit(X_train, y_train)\n",
    "y_pred = subjectivity_classifier.predict(X_test)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "#TODO: error?\n",
    "print(confusion_mat, classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. Topic Mining</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "def extract_topics_lda(words, doc_term_mat, number_topics = 5, number_words = 10):\n",
    "    # Create and fit the LDA model\n",
    "    lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    lda.fit(doc_term_mat)\n",
    "    # Print the topics found by the LDA model\n",
    "    topics = []\n",
    "    for topic in lda.components_:\n",
    "        topics.append([words[i] for i in topic.argsort()[:-number_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "def load_subjectivity_lexicon():\n",
    "    posUrl, negUrl = \"resources/sentiment_lexicon/positive-words.txt\", \"resources/sentiment_lexicon/negative-words.txt\"\n",
    "    with open(posUrl, \"r\") as posFile, open(negUrl, \"r\") as negFile:\n",
    "        posText, negText = posFile.read(), negFile.read()\n",
    "        posLines, negLines = posText.split(\"\\n\"), negText.split(\"\\n\")\n",
    "        return posLines[31:], negLines[31:]\n",
    "    \n",
    "positive_words, negative_words = load_subjectivity_lexicon()\n",
    "english_lemmatized_tokens = list(dataset.english_lemmatized_tokens)\n",
    "neutral_english_lemmatized_tokens = []\n",
    "for tweet in english_lemmatized_tokens:\n",
    "    arr = []\n",
    "    for token in tweet:\n",
    "        if not(token in positive_words or token in negative_words):\n",
    "            arr.append(token)\n",
    "    neutral_english_lemmatized_tokens.append(arr)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    min_df=50,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "tfidf.fit(neutral_english_lemmatized_tokens)\n",
    "\n",
    "pos_lemmatized_tokens = list(dataset[dataset.airline_sentiment==\"positive\"].english_lemmatized_tokens)\n",
    "pos_lemmatized_tokens\n",
    "pos_mat = tfidf.transform(pos_lemmatized_tokens)\n",
    "\n",
    "neg_lemmatized_tokens = dataset[dataset.airline_sentiment==\"negative\"].english_lemmatized_tokens\n",
    "neg_mat = tfidf.transform(neg_lemmatized_tokens)\n",
    "\n",
    "neutral_lemmatized_tokens = dataset[dataset.airline_sentiment==\"neutral\"].english_lemmatized_tokens\n",
    "neutral_mat = tfidf.transform(neutral_lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_topics = extract_topics_lda(tfidf.get_feature_names(), neg_mat)\n",
    "neutral_topics = extract_topics_lda(tfidf.get_feature_names(), neutral_mat)\n",
    "pos_topics = extract_topics_lda(tfidf.get_feature_names(), pos_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['bag',\n",
       "   'flight',\n",
       "   'late',\n",
       "   'luggage',\n",
       "   'late flight',\n",
       "   'still',\n",
       "   'connection',\n",
       "   'plane',\n",
       "   'make',\n",
       "   'happen'],\n",
       "  ['flight',\n",
       "   'airline',\n",
       "   'fly',\n",
       "   'never',\n",
       "   'gate',\n",
       "   'time',\n",
       "   'use',\n",
       "   'agent',\n",
       "   'ever',\n",
       "   'us'],\n",
       "  ['flight',\n",
       "   'cancel',\n",
       "   'cancel flight',\n",
       "   'get',\n",
       "   'flight cancel',\n",
       "   'help',\n",
       "   'tomorrow',\n",
       "   'need',\n",
       "   'go',\n",
       "   'weather'],\n",
       "  ['customer',\n",
       "   'service',\n",
       "   'customer service',\n",
       "   'call',\n",
       "   'phone',\n",
       "   'get',\n",
       "   \"can't\",\n",
       "   'help',\n",
       "   'answer',\n",
       "   'line'],\n",
       "  ['hour', 'hold', 'wait', 'minute', '2', 'flight', '3', 'min', 'sit', 'day']],\n",
       " [['thanks',\n",
       "   'much',\n",
       "   'yes',\n",
       "   'take',\n",
       "   'see',\n",
       "   'time',\n",
       "   'flight',\n",
       "   'make',\n",
       "   \"can't\",\n",
       "   'get'],\n",
       "  ['guy',\n",
       "   'response',\n",
       "   'thanks',\n",
       "   'would',\n",
       "   'know',\n",
       "   'quick',\n",
       "   'reply',\n",
       "   'find',\n",
       "   'go',\n",
       "   'never'],\n",
       "  ['flight',\n",
       "   'crew',\n",
       "   'follow',\n",
       "   'please',\n",
       "   'get',\n",
       "   'home',\n",
       "   'keep',\n",
       "   'thanks',\n",
       "   'gate',\n",
       "   'agent'],\n",
       "  ['airline',\n",
       "   'look',\n",
       "   'always',\n",
       "   'first',\n",
       "   'fly',\n",
       "   'day',\n",
       "   'staff',\n",
       "   'flight',\n",
       "   'way',\n",
       "   'make'],\n",
       "  ['service',\n",
       "   'customer',\n",
       "   'customer service',\n",
       "   'thanks',\n",
       "   'fly',\n",
       "   'team',\n",
       "   'hope',\n",
       "   'make',\n",
       "   'oh',\n",
       "   'give']],\n",
       " [['flight',\n",
       "   'tomorrow',\n",
       "   'cancel',\n",
       "   'chance',\n",
       "   'start',\n",
       "   'sent',\n",
       "   'year',\n",
       "   'snow',\n",
       "   'airport',\n",
       "   'travel'],\n",
       "  ['dm',\n",
       "   'please',\n",
       "   'yes',\n",
       "   'number',\n",
       "   'see',\n",
       "   '...',\n",
       "   'send',\n",
       "   'passenger',\n",
       "   'flight',\n",
       "   'look'],\n",
       "  ['get',\n",
       "   'go',\n",
       "   'fly',\n",
       "   'flight',\n",
       "   'use',\n",
       "   'would',\n",
       "   'time',\n",
       "   'next',\n",
       "   'new',\n",
       "   'still'],\n",
       "  ['hi',\n",
       "   'way',\n",
       "   'flight',\n",
       "   'ticket',\n",
       "   'leave',\n",
       "   'come',\n",
       "   'know',\n",
       "   'back',\n",
       "   'u',\n",
       "   'get'],\n",
       "  [\"fleet's\",\n",
       "   'follow',\n",
       "   'thanks',\n",
       "   'need',\n",
       "   'rt',\n",
       "   'help',\n",
       "   'change',\n",
       "   '...',\n",
       "   \"rt fleet's\",\n",
       "   'add']])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_topics, pos_topics, neutral_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. Load NFCU tweets</h1>\n",
    "\n",
    "- Normalize tweets in the same way as we normalized airline tweets\n",
    "    - Denoise, tokenize and lemmatize \n",
    "    - Vectorize using the same vocabulary\n",
    "- Apply subjectivity classifier and label tweets as subjective vs objective\n",
    "- Apply polarity classifier and label subjective tweets as positive or negative\n",
    "- Run LDA on objective tweets, subjective tweets, positive tweets and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../../data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDNew</th>\n",
       "      <th>SocialNetwork</th>\n",
       "      <th>SenderUserId</th>\n",
       "      <th>FollowersCount</th>\n",
       "      <th>Message</th>\n",
       "      <th>CreatedTime</th>\n",
       "      <th>MessageType</th>\n",
       "      <th>NormalizedMessage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.158647e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>1.150201e+18</td>\n",
       "      <td>99</td>\n",
       "      <td>Hey @NavyFederalHelp @NavyFederal are you guys...</td>\n",
       "      <td>2020-01-30 19:09:07.795</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>hey are you guys compatible with the security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.158647e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>1.150201e+18</td>\n",
       "      <td>99</td>\n",
       "      <td>Hey @NavyFederalHelp @NavyFederal are you guys...</td>\n",
       "      <td>2020-01-30 19:09:07.795</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>hey are you guys compatible with the security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.131474e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>8.765216e+17</td>\n",
       "      <td>407</td>\n",
       "      <td>@NavyFederal You’re welcome. Tons of locals wi...</td>\n",
       "      <td>2020-01-30 19:07:55.126</td>\n",
       "      <td>Twitter Reply</td>\n",
       "      <td>you are welcome tons of locals with nf in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.131474e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>8.765216e+17</td>\n",
       "      <td>407</td>\n",
       "      <td>@NavyFederal please put a location in Daytona ...</td>\n",
       "      <td>2020-01-30 19:01:02.803</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>please put a location in daytona thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.066183e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>9.404094e+08</td>\n",
       "      <td>1079</td>\n",
       "      <td>@NavyFederal i’m bout SICK of yall.</td>\n",
       "      <td>2020-01-30 17:53:31.435</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>i am bout sick of yall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.110529e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>1.465241e+09</td>\n",
       "      <td>109125</td>\n",
       "      <td>Davide Moretti drove his way through the lane ...</td>\n",
       "      <td>2020-01-30 17:20:00.257</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>davide moretti drove his way through the lane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.985451e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>4.194737e+08</td>\n",
       "      <td>3755</td>\n",
       "      <td>@TheNCUA how is it @NavyFederal  can disable a...</td>\n",
       "      <td>2020-01-30 17:09:14.505</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>how is it can disable all tabs and disable my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.162829e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>1.199326e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>@defenseupdate @IDFSpokesperson @chicagoGDC @H...</td>\n",
       "      <td>2020-01-30 17:00:29.424</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>defense apac pga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.985451e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>4.194737e+08</td>\n",
       "      <td>3755</td>\n",
       "      <td>Gee Navy Federal Is A Bank Of Be Best Aint it ...</td>\n",
       "      <td>2020-01-30 17:00:11.694</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>gee navy federal is a bank of be best aint it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.958612e+09</td>\n",
       "      <td>TWITTER</td>\n",
       "      <td>3.207328e+08</td>\n",
       "      <td>993</td>\n",
       "      <td>@mac_10k @NavyFederal We good here. Thanks but...</td>\n",
       "      <td>2020-01-30 16:37:24.573</td>\n",
       "      <td>Twitter Mention</td>\n",
       "      <td>k we good here thanks but no thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDNew SocialNetwork  SenderUserId  FollowersCount  \\\n",
       "0  4.158647e+09       TWITTER  1.150201e+18              99   \n",
       "1  4.158647e+09       TWITTER  1.150201e+18              99   \n",
       "2  4.131474e+09       TWITTER  8.765216e+17             407   \n",
       "3  4.131474e+09       TWITTER  8.765216e+17             407   \n",
       "4  2.066183e+09       TWITTER  9.404094e+08            1079   \n",
       "5  2.110529e+09       TWITTER  1.465241e+09          109125   \n",
       "6  1.985451e+09       TWITTER  4.194737e+08            3755   \n",
       "7  4.162829e+09       TWITTER  1.199326e+18               1   \n",
       "8  1.985451e+09       TWITTER  4.194737e+08            3755   \n",
       "9  1.958612e+09       TWITTER  3.207328e+08             993   \n",
       "\n",
       "                                             Message              CreatedTime  \\\n",
       "0  Hey @NavyFederalHelp @NavyFederal are you guys...  2020-01-30 19:09:07.795   \n",
       "1  Hey @NavyFederalHelp @NavyFederal are you guys...  2020-01-30 19:09:07.795   \n",
       "2  @NavyFederal You’re welcome. Tons of locals wi...  2020-01-30 19:07:55.126   \n",
       "3  @NavyFederal please put a location in Daytona ...  2020-01-30 19:01:02.803   \n",
       "4                @NavyFederal i’m bout SICK of yall.  2020-01-30 17:53:31.435   \n",
       "5  Davide Moretti drove his way through the lane ...  2020-01-30 17:20:00.257   \n",
       "6  @TheNCUA how is it @NavyFederal  can disable a...  2020-01-30 17:09:14.505   \n",
       "7  @defenseupdate @IDFSpokesperson @chicagoGDC @H...  2020-01-30 17:00:29.424   \n",
       "8  Gee Navy Federal Is A Bank Of Be Best Aint it ...  2020-01-30 17:00:11.694   \n",
       "9  @mac_10k @NavyFederal We good here. Thanks but...  2020-01-30 16:37:24.573   \n",
       "\n",
       "       MessageType                                  NormalizedMessage  \n",
       "0  Twitter Mention  hey are you guys compatible with the security ...  \n",
       "1  Twitter Mention  hey are you guys compatible with the security ...  \n",
       "2    Twitter Reply  you are welcome tons of locals with nf in the ...  \n",
       "3  Twitter Mention            please put a location in daytona thanks  \n",
       "4  Twitter Mention                             i am bout sick of yall  \n",
       "5  Twitter Mention  davide moretti drove his way through the lane ...  \n",
       "6  Twitter Mention  how is it can disable all tabs and disable my ...  \n",
       "7  Twitter Mention                                   defense apac pga  \n",
       "8  Twitter Mention  gee navy federal is a bank of be best aint it ...  \n",
       "9  Twitter Mention                k we good here thanks but no thanks  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
